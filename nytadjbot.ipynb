{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bot Skeleton"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up and authenticate Twitter account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PresAdjectives\n"
     ]
    }
   ],
   "source": [
    "import tweepy\n",
    "\n",
    "#Setup and authenticate Tweepy\n",
    "CONSUMER_KEY = \"\"\n",
    "CONSUMER_SECRET = \"\"\n",
    "ACCESS_TOKEN = \"\"\n",
    "ACCESS_TOKEN_SECRET = \"\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "auth.set_access_token(ACCESS_TOKEN, ACCESS_TOKEN_SECRET)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "me = api.me()\n",
    "print me.screen_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function for sending a tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import json\n",
    "import time\n",
    "\n",
    "# If debug_mode is True then the bot won't actually tweet. Set debug_mode = False for it to tweet. \n",
    "debug_mode = True\n",
    "\n",
    "# Functions that allow the bot to tweet or reply to tweets\n",
    "def tweet(status):\n",
    "    print \"JUST TWEETED: \", status\n",
    "    # Only *actually* send the tweet on twitter if we're not in debug mode\n",
    "    if debug_mode == False:\n",
    "        api.update_status(status)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell will actually run the bot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17 articles\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# The list of candidate names and Twitter handles. Twitter handles are used so that searching for the candidates will find the bot's tweets\n",
    "candidates = [[\"Donald\",\"Trump\", \"@realDonaldTrump\"],[\"Hillary\",\"Clinton\", \"@HillaryClinton\"],[\"Bernie\",\"Sanders\", \"@BernieSanders\"],\n",
    "              [\"Ted\",\"Cruz\", \"@TedCruz\"],[\"John\",\"Kasich\", \"@JohnKasich\"]]\n",
    "# Other terms to search the NYT website for\n",
    "other_terms = [\"Democrat\",\"Republican\"]\n",
    "\n",
    "# Create the string for searching the New York Times website. Only use last names so we don't find articles about other people with the same first names\n",
    "search_terms = [c[1] for c in candidates] + other_terms\n",
    "articles = []\n",
    "\n",
    "search_string = ' OR '.join(['headline:' + term for term in search_terms])\n",
    "\n",
    "# Only look at articles from the most recent day\n",
    "begin_date_string = time.strftime(\"%Y%m%d\")\n",
    "\n",
    "# Use the New York Times API to find all articles from the most recent day that mention the candidates\n",
    "response = requests.get('http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=%s&begin_date=%s&api-key=7a6bb7d7d596c658b07bb1ecf1dd485b:13:74696179' % (search_string, begin_date_string))\n",
    "html=response.text\n",
    "values = json.loads(html)\n",
    "\n",
    "new_articles = values['response']['docs']\n",
    "articles += new_articles\n",
    "\n",
    "# Find additional pages of results\n",
    "i = 2\n",
    "while len(new_articles) > 0:\n",
    "    response = requests.get('http://api.nytimes.com/svc/search/v2/articlesearch.json?fq=%s&begin_date=%s&page=%d&api-key=7a6bb7d7d596c658b07bb1ecf1dd485b:13:74696179' % (search_string, begin_date_string, i))\n",
    "    html=response.text\n",
    "    try:\n",
    "        values = json.loads(html)\n",
    "        \n",
    "        new_articles = values['response']['docs']\n",
    "        articles += new_articles\n",
    "    except Exception as e:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "print \"Found \" + str(len(articles)) + \" articles\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "{'Sanders': {}, 'Cruz': {}, 'Clinton': {}, 'Trump': {u'unique': 1}, 'Kasich': {}}\n",
      " Trump is unique\n",
      "[(u'Trump', 'NNP'), (u'is', 'VBZ'), (u'unique', 'JJ')]\n",
      " Trump is unique \n",
      "1\n",
      "2\n",
      "3\n",
      "{'Sanders': {}, 'Cruz': {}, 'Clinton': {}, 'Trump': {u'unique': 1, u'anti-semitic': 1}, 'Kasich': {}}\n",
      " Trump is anti-Semitic\n",
      "[(u'Trump', 'NN'), (u'is', 'VBZ'), (u'anti-Semitic', 'JJ')]\n",
      " Trump is anti-Semitic \n",
      "4\n",
      "5\n",
      "{'Sanders': {}, 'Cruz': {}, 'Clinton': {u'rival': 1}, 'Trump': {u'unique': 1, u'anti-semitic': 1}, 'Kasich': {}}\n",
      " Democratic presidential candidate Bernie Sanders outraised rival Hillary Clinton in February but spent at a faster pace\n",
      "[(u'Democratic', 'JJ'), (u'presidential', 'JJ'), (u'candidate', 'NN'), (u'Bernie', 'NNP'), (u'Sanders', 'NNP'), (u'outraised', 'VBD'), (u'rival', 'JJ'), (u'Hillary', 'NNP'), (u'Clinton', 'NNP'), (u'in', 'IN'), (u'February', 'NNP'), (u'but', 'CC'), (u'spent', 'VBN'), (u'at', 'IN'), (u'a', 'DT'), (u'faster', 'JJR'), (u'pace', 'NN')]\n",
      " Democratic presidential candidate Bernie Sanders outraised rival Hillary Clinton in February but spent at a faster pace \n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "JUST TWEETED:  @realDonaldTrump is 'white', 'unique', and 'anti-semitic'\n",
      "{'Sanders': {}, 'Cruz': {}, 'Clinton': {u'rival': 1}, 'Trump': {}, 'Kasich': {}}\n",
      " A white Trump supporter is shown sucker-punching a black protester at a rally\n",
      "[(u'A', 'DT'), (u'white', 'JJ'), (u'Trump', 'NNP'), (u'supporter', 'NN'), (u'is', 'VBZ'), (u'shown', 'VBN'), (u'sucker-punching', 'VBG'), (u'a', 'DT'), (u'black', 'JJ'), (u'protester', 'NN'), (u'at', 'IN'), (u'a', 'DT'), (u'rally', 'NN')]\n",
      " A white Trump supporter is shown sucker-punching a black protester at a rally \n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "{'Sanders': {}, 'Cruz': {}, 'Clinton': {u'rival': 1}, 'Trump': {}, 'Kasich': {}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda2\\lib\\site-packages\\bs4\\__init__.py:166: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
      "\n",
      "To get rid of this warning, change this:\n",
      "\n",
      " BeautifulSoup([your markup])\n",
      "\n",
      "to this:\n",
      "\n",
      " BeautifulSoup([your markup], \"lxml\")\n",
      "\n",
      "  markup_type=markup_type))\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "\n",
    "# Initialize the data structure for storing the adjectives found\n",
    "adjectives = {}\n",
    "for candidate in candidates:\n",
    "    adjectives[candidate[1]] = {}\n",
    "\n",
    "# Look through each article\n",
    "for (index, article) in enumerate(articles):\n",
    "    print index\n",
    "    \n",
    "    # Download the HTML of the article's webpage\n",
    "    response = requests.get(article['web_url'])\n",
    "    html = response.text\n",
    "\n",
    "    # Find the body text of the article by joining the data in each div with class 'story-content'\n",
    "    article_bs = BeautifulSoup(html)\n",
    "    article_text = \"\\n\".join([a.text for a in article_bs.find_all(class_=\"story-content\")])\n",
    "    \n",
    "    # Split the text by sentence\n",
    "    sentences = re.split('\\.|\\?|\\,', article_text)\n",
    "    \n",
    "    # Look through the sentences\n",
    "    for sentence in sentences:\n",
    "        for candidate in candidates:\n",
    "            # If a candidate is mentioned in the sentence\n",
    "            if candidate[0] in sentence or candidate[1] in sentence:\n",
    "                # Use a regular expression to remove punctuation on the outside of words and then split by word\n",
    "                # This should keep punctuation in cases such as contractions (isn't), but not, for example quotes (\"Hello\")\n",
    "                tokens = nltk.word_tokenize(re.sub('([^a-zA-Z0-9_ ]* [^a-zA-Z0-9_ ]*|^[^a-zA-Z0-9_ ]*|[^a-zA-Z0-9_ ]*$)+', ' ', sentence))\n",
    "                \n",
    "                # Use the Natural Language Toolkit to find the part of speech of each word in the sentence\n",
    "                pairs = nltk.pos_tag(tokens)\n",
    "                \n",
    "                # Look through word by word\n",
    "                for (i, pair) in enumerate(pairs):\n",
    "                    # If the word is an adjective, test if it is either in the form <adjective> <candidate> (e.g. \"great Hillary Clinton\")\n",
    "                    # or the form <candidate> \"is\"/\"was\" <adjective> (e.g. \"Ted Cruz is great\")\n",
    "                    if pair[1] == \"JJ\" and ((i < len(pairs) - 1 and pairs[i+1][0] in candidate)\n",
    "                                           or (i > 1 and pairs[i - 2][0] in candidate and pairs[i-1][0] in [\"is\", \"was\"])):\n",
    "                        # If the adjective has already been found, increment the count of how many times it's been found\n",
    "                        if pair[0].lower() in adjectives[candidate[1]]:\n",
    "                            adjectives[candidate[1]][pair[0].lower()] += 1\n",
    "                        # If it's new, add it to the candidate's list of adjectives\n",
    "                        else:\n",
    "                            adjectives[candidate[1]][pair[0].lower()] = 1\n",
    "                            # If a candidate has three distinct adjectives, tweet them and remove those adjectives from the list\n",
    "                            if len(adjectives[candidate[1]]) == 3:\n",
    "                                tweet(\"%s is '%s', '%s', and '%s'\" % (candidate[2], adjectives[candidate[1]].keys()[0],\n",
    "                                                                        adjectives[candidate[1]].keys()[1], adjectives[candidate[1]].keys()[2]))\n",
    "\n",
    "                                keys = adjectives[candidate[1]].keys()\n",
    "                                for key in keys:\n",
    "                                    del adjectives[candidate[1]][key]\n",
    "                        print adjectives\n",
    "                        print sentence\n",
    "                        print pairs\n",
    "                        print re.sub('([^a-zA-Z0-9_ ]* [^a-zA-Z0-9_ ]*|^[^a-zA-Z0-9_ ]*|[^a-zA-Z0-9_ ]*$)+', ' ', sentence)\n",
    "print adjectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
